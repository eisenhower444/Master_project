\chapter{Introduction}

\section{Deep Learning and Convolution Neural Networks}

\noindent Neural networks, or Deep Learning as it is now referred, is a field of artificial intelligence that has attracted a lot of attention recently \citep{NYT_article}. Over the last few years, they have become state of the art in a number of international contests in pattern recognition, particularly image recognition, speech recognition, and natural language processing\citep{Deep_Learning_overview}. \\

\noindent Most of the difficulties in applying Machine Learning techniques to real world problems comes from the high dimensionality of the datasets encountered, severely increasing the learning complexity of the algorithms required to handle them and thereby reducing their generalisation properties. Traditionally this problem is tackled by finding ways to reduce the number of dimensions in what is known as "feature extraction", a deliberate hand-engineered process of transforming the large number of variables into a smaller number of features with the same level of information content. By contrast, neural networks aim to learn the relevant features as part of the learning process itself, by training a hierarchical network of simple processing units called neurons or nodes on a large set of data in order to extract a good representation model of the data. Learning in this context means finding a set of weights which makes the neural network exhibit the desired behaviour (e.g. classify hand written-digits correctly).\\

\noindent The first neural networks with multiple, if few, layers date back to the 60s [reference] and have been around for decades. For feedforward neural networks, where the information can only one way through the network, an efficient gradient based method that is widely used called the backpropagation method was developed then and applied to neural networks in 1981 [reference]. However a number of practical difficulties made it difficult to train architectures with more a small number of layers. In particular, one common issue was the so-called neuron saturation problem [reference] where I need to explain what that means.A number of practical solutions where offered [reference], from the choice of parameter initialisation to the influence of the various possible activation functions on the learning behaviour of the network [reference]. However in 2006, a seminal paper [reference] by Hinton made training deep neural networks practical using an unsupervised pre-training approach. These networks called Deep Belief Networks, have lead to a resurgence in the interest in neural networks.\\

\noindent Another major recent development was the introduction of convolutional neural networks (CNNs) by LeCun [reference]. CNNs are a specialised type of Neural Network for certain grid-like structures with features that are strongly localised. In particular, they have been remarkably effective on classification tasks on images, videos, and audio recordings, providing state of the art performance on object and speech recognition tasks. They were introduced in 1998 in the context of hand-written digit recognition on the MNIST dataset. Ever since they have won many official international pattern recognition competitions, in particular the Imagenet challenge. A full historical account can be found in [reference].\\

\section{Deep Learning in Medical Imaging}

Medical Imaging is a set of techniques to create visual representations of the interior of the body using technologies such as X-rays or ultra-sounds. It's aim is to provide a non-invasive way of making diagnosis by revealing internal structures. Interpreting these images have traditionally been done by trained clinicians such as radiologist or histologists, involving a time consuming and expensive process. Current efforts are being made to automate this process [reference] in order to improve the accuracy, speed and cost of diagnoses.\\

Following the string of recent success of Deep Learning approaches in the various contexts, there has been a recent effort to implement these models in the context of Medical Imaging. These efforts include for instance the anatomical segmentation of the entire brain [reference], the segmentation of the lungs for cancer detection [reference], biological neuron membrane to map 3D brain structure and connectivity [reference], tibial cartilage [reference], detection of bone tissue in X-ray images [reference] and counting cell mitosis in histology images [reference] for cancer screening and assessment, amongst others.\\

The segmentation process represents a first step necessary for any automatic method of extracting information from an image. The machine learning approach is to train a model to classify each voxel into its corresponding anatomical region given a training set consisting of labelled medical images.\\

Automating this task would enable systematic segmentation of medical images on the fly as soon as these images are acquired. They could also be useful in the detection of anatomical abnormalities such in tumour detections.\\

\section{Goal of the Thesis}

This Master's thesis aims to segments a 

takes direct inspiration from the paper on brain segmentation, where the entire brain was segmented into 140 different anatomical regions. It will propose a deep convolution neural network for the automated segmentation of chest CT scans into atrium and non-atrium parts using a tri-planar multi-scale approach. \\

The second chapters discusses some background on neural networks and CNNs. The second chapter then describes the approach undertaken and presents a number of experimental results from model selection, the varying of dataset size and various sampling strategies. We will finish with some conclusions and a discussion of various other ways to improve upon our results.










